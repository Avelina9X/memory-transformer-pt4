train:
  batch_size_step: 1

finetune:
  wrapped_model: Qwen/Qwen1.5-0.5B-Chat
  frozen_params:
    - embed_tokens.weight
    - lm_head.weight

model:
  pooler_config:
    layer_select: [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24 ]